Run all scripts from working directory Maxim!!!

Dependencies:
    if you are using linux you can run dependencies.sh

    it creates empty directories:
        ./fasta
        ./genomes
        ./genes
        ./ncbi_tools
        ./blast_db
        ./proteins
        ./proteins_filtered
        ./proteins_aligned

    otherwise some dependencies need to be installed specifically for your operating system accordingly like:

    ncbi binaries to ./ncbi_tools
        https://ftp.ncbi.nlm.nih.gov/pub/datasets/command-line/LATEST/
        https://ftp.ncbi.nlm.nih.gov/pub/datasets/command-line/LATEST/

    aliview tar extracted to ./aliview
        https://www.ormbunkar.se/aliview/downloads/

    mafft
        use your system package manager or download the installer online


File Structure:

- aliview
    * aliview (executable)
    * ...
- blast_db
    (all genomes db generated by make
- genes
    (downloaded by scrape_genes.py)
    gene_summery.json
    - <taxid>
        - <accession_id>
            - ncbi_dataset
                - data
                    data_report.jsonl
                    dataset_catalog.json
                    protein.faa
                    ...
- genomes
    (downloaded by scrape_genomes.py)
    genome_summery.json
    - <taxid>
        - ncbi_dataset
            - data
                assembly_data_report.jsonl
                dataset_catalog.json
                - <id>
                    <id>.<...>_genomic.fna
                    genomic.gff3
- ncbi_tools
    dataformat
    datasets
- proteins
    (downloaded by scrape_proteins.py)
    <...>.fasta
- proteins_aligned
    (aligned by multiple_sequence_alignment.py)
    <...>.fasta
- proteins_filtered
    (filtered sequences by filter_proteins.py)
    <...>.fasta
dependencies.sh
blaststuff.py
filter_proteins.py
find_neighboring_genes_process.py
multiple_sequence_alignment.py
proteins_pipeline.py
scrape_genes.py
scrape_genome.py
scrape_proteins.py